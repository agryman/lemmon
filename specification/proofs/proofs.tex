\documentclass[11pt, oneside]{article}

\usepackage{../shared/preamble}
\addbibresource{../shared/references.bib}

\usepackage{proofs}

\title{Proofs}
\author{Arthur Ryman, {\tt arthur.ryman@gmail.com}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This article is a Z Notation specification for proofs and proof checking.
It has been type checked by \fuzz.
The definitions that appear here are taken from Lemmon's book, {\it Beginning Logic}.
The purpose of this specification is to guide the development of a proof checker
aimed at Z specifications
\end{abstract}

\section{Introduction}

For a long time I have thought that it would be extremely useful to be able to write formal proofs
concerning the mathematical objects defined in Z specifications.
There are some very mature proof assistants available.
I know something about Coq, but unfortunately it's style of proof is very different from that one finds in mathematical
papers.

A Coq proof consists of applications of so-called {\it tactics}.
Each tactic represents a higher level aggregate of deductions.
This makes sense for a proof assistant since its job is to help the user discover proofs.
Tactics are like macros and as such they alleviate the user from much low-level tedium.
However, the analog of a macro in normal mathematical writing is a lemma.
Perhaps tactics are more useful for proving formal properties of programming languages where the mathematical
objects of interest are complex, but finite, recursive structures.

While a proof assistant might make sense is some contexts, proper development of a mathematical paper
consists of a gradual introduction of concepts and lemmas leading to the main results.
The proofs should, in some sense, write themselves.
The focus of a mathematical paper should be on explanation and clarity.
The proofs should be easy to read.
I'd therefore really like something that would let me write and check natural looking proofs.

In contrast to Coq, the style of proof presented by Lemmon is very clear and explicit.
However, the task of checking such a proof could easily be delegated to a program,
much the same way that \fuzz\ type checks Z.
In fact Lemmon makes that point that, although there is no mechanical way to discover proofs,
they can be mechanically checked.

I believe that the kernel of a proof checker could be very small.
It is basically an engine driven by a set of deduction rules.
The engine simply needs to check that each deduction rule gets applied correctly.
Even if it turns out that writing such an engine is too much work, the exercise of developing
at least a simple version should give me a greater appreciation of tools like Coq and enable me to
use them more productively.

My plan of attack is to formalize the concept of proof as described by Lemmon, starting with
the propositional calculus, and then move on to predicate calculus.

\section{The Propositional Calculus 1}

The {\it propositional calculus} defines a set of formal {\it statements} or {\it propositions}
without being concerned about the subject matter described by those statements.
The only restriction on these statements is that, in any given context, 
they possess a {\it truth value} of either {\it true} or {\it false}.

A proposition is also referred to as a {\it well-formed formula} or {\it wff} for short.
This terminology stems from the traditional development of the propositional calculus in terms of
a language of sentences over an alphabet with rules that prescribe when a given sentence is {\it well-formed}.
However, here we will dispense with the language viewpoint, and its associated parsing issues, and move directly
to the end result of parsing, namely the creation of {\it abstract syntax trees} or {\it ASTs} for short.
This approach corresponds to Lemmon's Chapter 1.
He returns to the issue of formal languages in Chapter 2.

\subsection{$Prop$}


Z notation has a convenient mechanism for specifying the structure of ASTs, namely that of {\it free types}.
My first impulse was to use that mechanism to define propositions.
However, the problem with free types is that they are {it closed} in the sense that all ways of constructing members of a free type
must be specified when the free type is defined.
Lemmon's book gradually introduces ways of constructing propositions, so in the interest of following his development of the subject
as closely as possible, I won't define propositions that way.

On closer examination of free types in Z, it will be observed that they are merely syntactic sugar for introducing a new given set
along with an exhaustive set of constructors for its elements.
The constraint expressing the condition that the constructors are exhaustive is equivalent to saying that the domains of the constructors
partition the set of propositions.

Therefore, given any subset of constructors, one can define the corresponding set of propositions that can be constructed from them.
This allows new constructors to be gradually introduced.
I'll take that approach.

Let $Prop$ denote the set of all propositions.

\begin{zed}
	[Prop]
\end{zed}

\subsection{$PropVar$}

The separation of the form of a statement from its content with respect to any given subject matter is accomplished by 
the use of {\it propositional variables}. 
A propositional variable stands for an arbitrary statement that is either true or false.
Let $PropVar$ denote the set of propositional variables.

\begin{axdef}
	PropVar: \power Prop
\end{axdef}

\subsection{\zcmd{propP}, \zcmd{propQ}, \zcmd{propR}, \zcmd{propS}, and \zcmd{propT}}

Traditionally, arbitrary statements are represented by single letters such as
$\propP$, $\propQ$, $\propR$, $\propS$, and $\propT$ which represent distinct propositions.

\begin{axdef}
	\propP, \propQ, \propR, \propS, \propT: PropVar
\where
	\disjoint \langle \{\propP\}, \{\propQ\}, \{\propR\}, \{\propS\}, \{\propT\} \rangle
\end{axdef}

\subsection{$PropLetter$}

Let $PropLetter$ denote the set of all proposition letters.

\begin{zed}
	PropLetter == \{ \propP, \propQ, \propR, \propS, \propT \}
\end{zed}

\subsection{\zcmd{propPrime}}

Typical propositions contain a small number of distinct statements, in which case the letters can be used.
If more statements occur then the letters are decorated with one or more primes, 
e.g. $\propP \propPrime$, $\propQ \propPrime \propPrime$, etc.
Appending a prime to a propositional variable is an injection from $PropVar$ to $PropVar$.

\begin{axdef}
	\_ \propPrime: PropVar \inj PropVar
\end{axdef}

\begin{example}
$\propP \propPrime$ and $\propQ \propPrime \propPrime$ are propositional variables.
\begin{zed}
	\propP \propPrime \in PropVar
\also
	\propQ \propPrime \propPrime \in PropVar
\end{zed}

\end{example}

A propositional variable is either a letter or is primed.

\begin{zed}
	\langle PropLetter, \ran (\_ \propPrime) \rangle \partition PropVar
\end{zed}

\subsection{\zcmd{notProp}}

Let $A$ be a proposition.
Let $\notProp A$ denote the {\it negation} of $A$.

\begin{axdef}
	\notProp: Prop \inj Prop
\end{axdef}

\begin{example}
$\notProp \propP$ is a negation.

\begin{zed}
	\notProp \propP \in Prop
\end{zed}

\end{example}

\subsection{$Negation$}

Let $Negation$ denote that set of all negations.

\begin{zed}
	Negation == \ran \notProp
\end{zed}

\subsection{\zcmd{andProp}}

Let $A$ and $B$ be propositions.
Let $A \andProp B$ denote the {\it conjunction} of $A$ and $B$.

\begin{axdef}
	\_ \andProp \_: Prop \cross Prop \inj Prop
\end{axdef}

\subsection{$Conjunction$}

Let $Conjunction$ denote the set of all conjunctions.

\begin{zed}
	Conjunction == \ran (\_ \andProp \_)
\end{zed}

\subsection{\zcmd{orProp}}

Let $A$ and $B$ be propositions.
Let $A \orProp B$ denote the {\it disjunction} of $A$ and $B$.

\begin{axdef}
	\_ \orProp \_: Prop \cross Prop \inj Prop
\end{axdef}

\subsection{$Disjunction$}

Let $Disjunction$ denote the set of all disjunctions.

\begin{zed}
	Disjunction == \ran (\_ \orProp \_)
\end{zed}

\subsection{\zcmd{impliesProp}}

Let $A$ and $B$ be propositions.
Let $A \impliesProp B$ denote the {\it conditional} of $A$ and $B$.

\begin{axdef}
	\_ \impliesProp \_: Prop \cross Prop \inj Prop
\end{axdef}

\subsection{$Conditional$}

Let $Conditional$ denote the set of all conditionals.

\begin{zed}
	Conditional == \ran (\_ \impliesProp \_)
\end{zed}

\subsection{\zcmd{equivProp}}

Let $A$ and $B$ be propositions.
Let $A \equivProp B$ denote the {\it biconditional} of $A$ and $B$.

\begin{axdef}
	\_ \equivProp \_: Prop \cross Prop \inj Prop
\end{axdef}

\subsection{$Biconditional$}

\begin{zed}
	Biconditional == \ran (\_ \equivProp \_)
\end{zed}

\section{Proofs}

Lemmon has a nice way of presenting proofs.
Here's an example.

\vspace{1ex}

$\mathbf{1}\ P \implies Q, P \vdash Q$

\vspace{1ex}

\begin{tabular}{l l r l l}
&	1	&	(1)	&	$P \implies Q$	&	A \\
&	2	&	(2)	&	$P$			&	A \\
&	1,2	&	(3)	&	$Q$			&	1,2 MPP
\end{tabular}

\vspace{1ex}

A proof consists of a {\it sequent} to be proved and a finite sequence of one or more {\it lines}.
The lines are labelled by consecutive natural numbers, starting at $1$.
Each line contains a proposition and the application of the {\it proof rule} used to add it to the proof. 
A proof rule is either an {\it assumption} or a {\it derivation}.
The {\it rule of assumption} allows the addition of any proposition.
The {\it rules of derivation} allow the addition of a {\it conclusion} derived from {\it premises} that have been
previously added.
A premise is therefore either an assumption or the conclusion of a previous deduction.
Every line of the proof can therefore be traced back to a finite, possibly empty, set of assumptions upon
which it ultimately {\it depends}.

\subsection{$ProofRule$}

Let $ProofRule$ denote the set of proof rule applications.

\begin{syntax}
	ProofRule	& ::= & \ruleA \\
			&  |	& \ruleMPP \ldata \nat_1 \cross \nat_1 \rdata
\end{syntax}

\begin{example}
$\ruleA$ and $\ruleMPP(1,2)$are proof rules applications.

\begin{zed}
	\ruleA \in ProofRule
\also
	\ruleMPP(1,2) \in ProofRule
\end{zed}

\end{example}

\subsection{$ProofLine$}

A proof line consists of a finite, possibly empty, set of assumptions, a proposition,
and the proof rule application used to derive the proposition.
Let $ProofLine$ denote the set of all proof lines.

\begin{schema}{ProofLine}
	assumptions: \finset \nat_1 \\
	prop: Prop \\
	rule: ProofRule
\end{schema}

\subsection{$ProofLineTuple$}

Let $ProofLineTuple$ denote the tuple formed by the components of a proof line.

\begin{zed}
	ProofLineTuple == \finset \nat_1 \cross Prop \cross ProofRule
\end{zed}

\begin{example}
Line 3 of proof $\mathbf{1}$ corresponds to the following proof line.

\begin{zed}
	(\{ 1, 2 \}, \propQ, \ruleMPP(1,2)) \in ProofLineTuple
\end{zed}

\end{example}

\subsection{\zcmd{proofLineTuple}}

Let $\proofLineTuple$ map a proof line to its tuple.

\begin{zed}
	\proofLineTuple == (\lambda ProofLine @ (assumptions, prop, rule))
\end{zed}

\begin{remark}
The mapping from proof lines to tuples is a bijection.

\begin{zed}
	\proofLineTuple \in ProofLine \bij ProofLineTuple
\end{zed}

\end{remark}

\subsection{$Argument$}

An {\it argument} is a finite sequence of proof lines.
Let $Argument$ denote the set of all arguments.

\begin{zed}
	Argument == \seq ProofLineTuple
\end{zed}

\subsection{$Proof_1$}

Let $Proof_1$ denote the argument defined by proof $\mathbf{1}$.

\begin{zed}
	Proof_1 == \\
	\t1	\{ 1 \mapsto (\{ 1\}, \propP \impliesProp \propQ, \ruleA), \\
	\t1	2 \mapsto (\{ 2 \}, \propP, \ruleA), \\
	\t1	3 \mapsto (\{ 1, 2 \}, \propQ, \ruleMPP(1,2)) \}
\end{zed}

\begin{example}
$Proof_1$ is an argument.

\begin{zed}
	Proof_1 \in Argument
\end{zed}

\end{example}

\subsection{$ArgumentLine$}

A argument is said to be {\it sound} or {\it valid} if each proof line in the argument
obeys the rules of derivation.
When assessing the validity of a proof line, we need to specify its line number within
the argument and examine its contents.
Let $ArgumentLine$ denote the set of all arguments and line numbers within it.

\begin{schema}{ArgumentLine}
	argument: Argument \\
	lineNumber: \nat_1
\where
	lineNumber \in \dom argument
\end{schema}

\begin{itemize}
\item The line number corresponds to a proof line within the argument.
\end{itemize}

\begin{example}
Line 3 in contained in proof $\mathbf{1}$.

\begin{zed}
	\LET argument == Proof_1; lineNumber == 3 @ \\
	\t1	ArgumentLine
\end{zed}

\end{example}

\subsection{$RuleOfAssumption$}

A proof line that uses the rule of assumption $\ruleA$ in an argument is sound if it depends only on itself.
Let $RuleOfAssumption$ denote the set of all argument lines that use the rule of assumption soundly.

\begin{schema}{RuleOfAssumption}
	ArgumentLine \\
	P: Prop
\where
	argument(lineNumber) = (\{ lineNumber \}, P,  \ruleA)
\end{schema}

\begin{itemize}
\item The line depends only on itself and the rule is the rule of assumptions $\ruleA$.
\end{itemize}

\subsection{$AssumptionArgumentLine$}

Let $AssumptionArgumentLine$ denote the set of all argument lines that are sound applications
of the rule of assumption.

\begin{zed}
	AssumptionArgumentLine \defs RuleOfAssumption \project ArgumentLine
\end{zed}

\begin{example}
Lines 1 and 2 of $Proof_1$ are sound applications of the rule of assumption.

\begin{zed}
	\LET	argument == Proof_1 @ \\
	\t1	\forall lineNumber : \{ 1, 2 \} @ \\
	\t2		AssumptionArgumentLine
\end{zed}

\end{example}

\subsection{$RuleOfMPP$}

A proof line that uses the rule $\ruleMPP(i,j)$ in an argument is sound if $i$ and $j$ are lines
that precede it, the proposition on line $i$ is an implication, the proposition on line $j$ is the antecedent
of the implication, the proposition of the proof line is the consequent of the implication,
and the proof line's assumptions is the union of the assumptions of lines $i$ and $j$.
Let $RuleOfMPP$ denote the set of all argument lines that use the rule of MPP soundly.

\begin{schema}{RuleOfMPP}
	ArgumentLine \\
	i, j: \nat_1 \\
	ProofLine_1 \\
	ProofLine_2 \\
	P, Q: Prop
\where
	i < lineNumber \land j < lineNumber
\also
	argument(i) = (assumptions_1, P \impliesProp Q, rule_1)
\also
	argument(j) = (assumptions_2, P, rule_2)
\also
	argument(lineNumber) = (assumptions_1 \cup assumptions_2, Q, \ruleMPP(i,j))
\end{schema}

\subsection{$MPPArgumentLine$}

Let $MPPArgumentLine$ denote the set of all argument lines that are sound applications
of the rule of MPP.

\begin{zed}
	MPPArgumentLine \defs RuleOfMPP \project ArgumentLine
\end{zed}

\begin{example}
Line 3 of $Proof_1$ is a sound application of the rule of MPP.

\begin{zed}
	\LET argument == Proof_1; \\
	\t1	lineNumber == 3 @ \\
	\t2		MPPArgumentLine
\end{zed}

\end{example}

\subsection{$SoundArgumentLine$}

An argument line is sound if it is a sound application of one of the rules of derivation.
Let $SoundArgumentLine$ denote the set of all sound argument lines.

\begin{zed}
	SoundArgumentLine \defs \\
	\t1	AssumptionArgumentLine \lor \\
	\t1	MPPArgumentLine
\end{zed}

\subsection{$Proof$}

A proof is an argument in which every line is sound.
Let $Proof$ denote the set of proofs.

\begin{zed}
	Proof == \{~ argument: Argument | \\
	\t1	\forall lineNumber: \dom argument @ \\
	\t2		SoundArgumentLine ~\}
\end{zed}

\begin{example}
$Proof_1$ is a proof.

\begin{zed}
	Proof_1 \in Proof
\end{zed}

\end{example}

\subsection{$Proof_2$}

Here is Lemmon's proof $\mathbf{2}$.

\vspace{1ex}

$\mathbf{2}\ \lnot Q \implies (\lnot P \implies Q), \lnot Q \vdash \lnot P \implies Q$

\vspace{1ex}

\begin{tabular}{l l r l l}
&	1	&	(1)	&	$\lnot Q \implies (\lnot P \implies Q)$	&	A \\
&	2	&	(2)	&	$\lnot Q$						&	A \\
&	1,2	&	(3)	&	$ \lnot P \implies Q$				&	1,2 MPP
\end{tabular}

\vspace{1ex}

Let $Proof_2$ denote this proof.

\begin{zed}
	Proof_2 == \\
	\t1	\{ 1 \mapsto (\{ 1\}, \notProp \propQ \impliesProp (\notProp \propP \impliesProp \propQ), \ruleA), \\
	\t1	2 \mapsto (\{ 2 \}, \notProp \propQ, \ruleA), \\
	\t1	3 \mapsto (\{ 1, 2 \}, \notProp \propP \impliesProp \propQ, \ruleMPP(1,2)) \}
\end{zed}

\begin{example}
$Proof_2$ is a proof.

\begin{zed}
	Proof_2 \in Proof
\end{zed}

\end{example}

\section{The Curry-Howard Correspondence}

The {\it Curry-Howard Correspondence} is an interpretation of propositions and proofs in terms of types.
To each proposition there correspondences a type.
The inhabitants of the type that corresponds to a proposition are proofs of that proposition.
The logical connectives that build up propositions correspond to type constructors.

To illustrate the correspondence, consider the implication logical connective $P \implies Q$.
It corresponds to the type constructor $P \fun Q$.
A proof $f$ of $P \implies Q$ corresponds to a function that maps any proof of $P$ to some proof of $Q$.

Of course, $P \fun Q$ is not a type in Z. 
It is a subset of the type $\power(P \cross Q)$.
Nevertheless, we'll press on and pretend that it is a Z type for now.

The rules of derivation correspond to the construction of a proof from assumptions.
For example the rule of MPP corresponds to function application.
Consider $Proof_1$.
It corresponds to the following.

\begin{schema}{CHProof1}[P,Q]
	f: P \fun Q \\
	x: P \\
	y: Q
\where
	y = f(x)
\end{schema}

\printbibliography

\end{document}