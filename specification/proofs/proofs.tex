\documentclass[11pt, oneside]{article}

\usepackage{../shared/preamble}
\addbibresource{../shared/references.bib}

\usepackage{proofs}

\title{Proofs}
\author{Arthur Ryman, {\tt arthur.ryman@gmail.com}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This article is a Z Notation specification for proofs and proof checking.
It has been type checked by \fuzz.
The definitions that appear here are taken from Lemmon's book, {\it Beginning Logic}.
The purpose of this specification is to guide the development of a proof checker
aimed at Z specifications
\end{abstract}

\section{Introduction}

For a long time I have thought that it would be extremely useful to be able to write formal proofs
concerning the mathematical objects defined in Z specifications.
There are some very mature proof assistants available.
I know something about Coq, but unfortunately it's style of proof is very different that that one finds in mathematical
papers.
A Coq proof consists of a list of tactics that represent higher level aggregates of deductions.
This makes sense for a proof assistant since its job is to help the user discover proofs.

While a proof assistant might make sense is some contexts, proper development of a mathematical paper
consists of a gradual introduction of concepts and lemmas leading to the main results.
The proofs should, in some sense, write themselves.
The focus of a mathematical paper should be on explanation and clarity.
The proofs should be easy to read.
I'd therefore really like something that would let me write and check natural looking proofs.

In contrast to Coq, the style of proof presented by Lemmon is very clear and explicit.
However, the task of checking such a proof could easily be delegated to a program,
much the same way that \fuzz\ type checks Z.

I believe that the kernel of a proof checker could be very small.
It is basically an engine driven by a set of deduction rules.
The engine simply needs to check that each deduction rule gets applied correctly.
Even if it turns out that writing such an engine is too much work, the exercise of developing
at least a simple version should give me a greater appreciation of tools like Coq and enable me to
use them more productively.

My plan of attack is to formalize the concept of proof as described by Lemmon, starting with
the propositional calculus.

\section{Propositions}

The {\it propositional calculus} defines a set of formal {\it statements} or {\it propositions}
without being concerned about the subject matter described by those statements.
The only restriction on these statements is that, in any given context, 
they possess a {\it truth value} of either {\it true} or {\it false}.

\subsection{$Prop$}

A proposition is also referred to as a {\it well-formed formula} or {\it wff} for short.
This terminology stems from the traditional development of the propositional calculus in terms of
a language of sentences over an alphabet with rules that prescribe when a given sentence is {\it well-formed}.
However, here we will dispense with the language viewpoint, and its associated parsing issues, and move directly
to the end result of parsing, namely the creation of {\it abstract syntax trees} or {\it ASTs} for short.

Z notation has a convenient mechanism for specifying the structure of ASTs, namely that of {\it free types}.
Let $Prop$ denote the free type of all propositions of the propositional calculus.
The free type definition of $Prop$ is as follows.

\begin{syntax}
	PropVar	& ::=	& \propP | \propQ | \propR | \propS | \propT \\
			&  | 	& (\_ \propPrime) \ldata PropVar \rdata
\also
	Prop		& ::= & \trueProp | \falseProp \\
			&  |	& \varProp \ldata PropVar \rdata \\
			&  |	& \notProp \ldata Prop \rdata \\
			&  |	& (\_ \andProp \_) \ldata Prop \cross Prop \rdata \\
			&  |	& (\_ \orProp \_) \ldata Prop \cross Prop \rdata \\
			&  |	& (\_ \impliesProp \_) \ldata Prop \cross Prop \rdata \\
			&  |	& (\_ \equivProp \_) \ldata Prop \cross Prop \rdata
\end{syntax}

Note that the definition of $Prop$ depends on the definition of $PropVar$, the set of propositional variables.
There are two {\it constant} propositions, namely $\trueProp$ and $\falseProp$.
Any propositional variable defines a proposition.
Propositions are built up recursively from the constants and variables using {\it logical connectives}.
Here I use logical connective symbols defined by Z rather than those defined by Lemmon.

\subsection{$PropVar$}

The separation of the form of a statement from any given subject matter is accomplished by 
the use of {\it propositional variables} that stand for arbitrary statements.
Let $PropVar$ denote the free type of all propositional variables.

\begin{remark}
There are a countable infinity of propositional variables,
e.g. $\propP, \propP\propPrime, \propP\propPrime\propPrime, \ldots$.

\begin{zed}
	PropVar \bij \nat \neq \emptyset
\end{zed}

\end{remark}

\subsection{\zcmd{propP}, \zcmd{propQ}, \zcmd{propR}, \zcmd{propS}, and \zcmd{propT}}

Traditionally, arbitrary statements are represented by single letters such as
$\propP$, $\propQ$, $\propR$, $\propS$, and $\propT$.

\begin{remark}
Each of $\propP$, $\propQ$, $\propR$, $\propS$, and $\propT$ is a propositional variable.

\begin{zed}
	\{ \propP, \propQ, \propR, \propS, \propT \} \subset PropVar
\end{zed}

\end{remark}

\subsection{\zcmd{propPrime}}

Typical propositions contain a small number of distinct statements, in which case the letters can be used.
If more statements occur then the letters are decorated with one or more primes, 
e.g. $\propP \propPrime$, $\propQ \propPrime \propPrime$, etc.

\begin{example}
$\propP \propPrime$ and $\propQ \propPrime \propPrime$ are propositional variables.
\begin{zed}
	\propP \propPrime \in PropVar
\also
	\propQ \propPrime \propPrime \in PropVar
\end{zed}

\end{example}

\begin{remark}
Appending a prime to a propositional variable is an injection from $PropVar$ to $PropVar$.

\begin{zed}
	(\_ \propPrime) \in PropVar \inj PropVar
\end{zed}

\end{remark}

\subsection{\zcmd{trueProp} and \zcmd{falseProp}}

Let $\trueProp$ denote the proposition that is true in all contexts and
let $\falseProp$ denote the proposition that is false in all contexts.
The propositional $\trueProp$ and $\falseProp$ are said to be {\it constant} because they do not
depend on the context.

\subsection{\zcmd{varProp}}

Let $V$ be a propositional variable.
Let $\varProp(V)$ denote the proposition defined by $V$.

\begin{remark}
The sets $PropVar$ and $Prop$ are different types.
The expression $\propP$ is a propositional variable and the expression $\varProp(\propP)$ is a proposition.

\begin{zed}
	\propP \in PropVar
\also
	\varProp(\propP) \in Prop
\end{zed}

\end{remark}

\subsection{\zcmd{notProp}}

Let $A$ be a proposition.
Let $\notProp A$ denote the {\it negation} of $A$.

\begin{example}
$\notProp(\varProp \propP)$ is a proposition.

\begin{zed}
	\notProp(\varProp \propP) \in Prop
\end{zed}

\end{example}

\subsection{\zcmd{notPropV}}

We can simplify the notation for negating a proposition defined by a propositional variable 
by defining a function that directly negates the propositional variable and produces a proposition.

\begin{axdef}
	\notPropV: PropVar \inj Prop
\where
	\forall V: PropVar @ \\
	\t1	\notPropV V = \notProp(\varProp V)
\end{axdef}

\subsection{\zcmd{andProp}}

Let $A$ and $B$ be propositions.
Let $A \andProp B$ denote the {\it conjunction} of $A$ and $B$.

\subsection{\zcmd{andPropVP}, \zcmd{andPropPV}, and \zcmd{andPropVV}}

We can simplify the notation for conjunctions involving propositions defined by propositional variables as follows.

\begin{axdef}
	\_ \andPropVP \_: PropVar \cross Prop \inj Prop \\
	\_ \andPropPV \_: Prop \cross PropVar \inj Prop \\
	\_ \andPropVV \_: PropVar \cross PropVar \inj Prop
\where
	\forall V: PropVar; A: Prop @ \\
	\t1	V \andPropVP A = (\varProp V) \andProp A
\also
	\forall A: Prop; V: PropVar @ \\
	\t1	A \andPropPV V = A \andProp (\varProp V)
\also
	\forall V, W: PropVar @ \\
	\t1	V \andPropVV W = (\varProp V) \andProp (\varProp W)
\end{axdef}

\subsection{\zcmd{orProp}}

Let $A$ and $B$ be propositions.
Let $A \orProp B$ denote the {\it disjunction} of $A$ and $B$.

\subsection{\zcmd{orPropVP}, \zcmd{orPropPV}, and \zcmd{orPropVV}}

We can simplify the notation for disjunctions involving propositions defined by propositional variables as follows.

\begin{axdef}
	\_ \orPropVP \_: PropVar \cross Prop \inj Prop \\
	\_ \orPropPV \_: Prop \cross PropVar \inj Prop \\
	\_ \orPropVV \_: PropVar \cross PropVar \inj Prop
\where
	\forall V: PropVar; A: Prop @ \\
	\t1	V \orPropVP A = (\varProp V) \orProp A
\also
	\forall A: Prop; V: PropVar @ \\
	\t1	A \orPropPV V = A \orProp (\varProp V)
\also
	\forall V, W: PropVar @ \\
	\t1	V \orPropVV W = (\varProp V) \orProp (\varProp W)
\end{axdef}

\subsection{\zcmd{impliesProp}}

Let $A$ and $B$ be propositions.
Let $A \impliesProp B$ denote the {\it implication} of $A$ and $B$.

\subsection{\zcmd{impliesPropVP}, \zcmd{impliesPropPV}, and \zcmd{impliesPropVV}}

We can simplify the notation for implications involving propositions defined by propositional variables as follows.

\begin{axdef}
	\_ \impliesPropVP \_: PropVar \cross Prop \inj Prop \\
	\_ \impliesPropPV \_: Prop \cross PropVar \inj Prop \\
	\_ \impliesPropVV \_: PropVar \cross PropVar \inj Prop
\where
	\forall V: PropVar; A: Prop @ \\
	\t1	V \impliesPropVP A = (\varProp V) \impliesProp A
\also
	\forall A: Prop; V: PropVar @ \\
	\t1	A \impliesPropPV V = A \impliesProp (\varProp V)
\also
	\forall V, W: PropVar @ \\
	\t1	V \impliesPropVV W = (\varProp V) \impliesProp (\varProp W)
\end{axdef}

\subsection{\zcmd{equivProp}}

Let $A$ and $B$ be propositions.
Let $A \equivProp B$ denote the {\it equivalence} of $A$ and $B$.

\subsection{\zcmd{equivPropVP}, \zcmd{equivPropPV}, and \zcmd{equivPropVV}}

We can simplify the notation for equivalences involving propositions defined by propositional variables as follows.

\begin{axdef}
	\_ \equivPropVP \_: PropVar \cross Prop \inj Prop \\
	\_ \equivPropPV \_: Prop \cross PropVar \inj Prop \\
	\_ \equivPropVV \_: PropVar \cross PropVar \inj Prop
\where
	\forall V: PropVar; A: Prop @ \\
	\t1	V \equivPropVP A = (\varProp V) \equivProp A
\also
	\forall A: Prop; V: PropVar @ \\
	\t1	A \equivPropPV V = A \equivProp (\varProp V)
\also
	\forall V, W: PropVar @ \\
	\t1	V \equivPropVV W = (\varProp V) \equivProp (\varProp W)
\end{axdef}

\section{Proofs}

Lemmon has a nice way of presenting proofs.
Here's an example.

\vspace{1ex}

$\mathbf{1}\ P \implies Q, P \vdash Q$

\vspace{1ex}

\begin{tabular}{l l r l l}
&	1	&	(1)	&	$P \implies Q$	&	A \\
&	2	&	(2)	&	$P$			&	A \\
&	1,2	&	(3)	&	$Q$			&	1,2 MPP
\end{tabular}

\vspace{1ex}

A proof consists of a {\it sequent} to be proved and a finite sequence of {\it lines}.
The lines are labelled by consecutive natural numbers, starting at $1$.
Each line contains a proposition and the application of the {\it proof rule} used to add it to the proof. 
A proof rule is either an {\it assumption} or a {\it derivation}.
The {\it rule of assumption} allows the addition of any proposition.
The {\it rules of derivation} allow the addition of a {\it conclusion} derived from {\it premises} that have been
previously added.
A premise is therefore either an assumption or the conclusion of a previous deduction.
Every line of the proof can therefore be traced back to a finite, possibly empty, set of assumptions upon
which it ultimately {\it depends}.

\subsection{$ProofRule$}

Let $ProofRule$ denote the set of proof rule applications.

\begin{syntax}
	ProofRule	& ::= & \ruleA \\
			&  |	& \ruleMPP \ldata \nat_1 \cross \nat_1 \rdata
\end{syntax}

\begin{example}
$\ruleA$ and $\ruleMPP(1,2)$are proof rules applications.

\begin{zed}
	\ruleA \in ProofRule
\also
	\ruleMPP(1,2) \in ProofRule
\end{zed}

\end{example}

\subsection{$ProofLine$}

A proof line consists of a finite, possibly empty, set of assumptions, a proposition,
and the proof rule application used to derive the proposition.
Let $ProofLine$ denote the set of all proof lines.

\begin{schema}{ProofLine}
	assumptions: \finset \nat_1 \\
	prop: Prop \\
	rule: ProofRule
\end{schema}

\subsection{$ProofLineTuple$}

Let $ProofLineTuple$ denote the tuple formed by the components of a proof line.

\begin{zed}
	ProofLineTuple == \finset \nat_1 \cross Prop \cross ProofRule
\end{zed}

\begin{example}
Line 3 of proof $\mathbf{1}$ corresponds to the following proof line.

\begin{zed}
	(\{ 1, 2 \}, \varProp(\propQ), \ruleMPP(1,2)) \in ProofLineTuple
\end{zed}

\end{example}

\subsection{\zcmd{proofLineTuple}}

Let $\proofLineTuple$ map a proof line to its tuple.

\begin{zed}
	\proofLineTuple == (\lambda ProofLine @ (assumptions, prop, rule))
\end{zed}

\begin{remark}
The mapping from proof lines to tuples is a bijections.

\begin{zed}
	\proofLineTuple \in ProofLine \bij ProofLineTuple
\end{zed}

\end{remark}

\subsection{$Argument$}

An {\it argument} is a finite sequence of proof lines.
Let $Argument$ denote the set of all arguments.

\begin{zed}
	Argument == \seq ProofLineTuple
\end{zed}

\subsection{$Proof_1$}

Let $Proof_1$ denote the argument defined by proof $\mathbf{1}$.

\begin{zed}
	Proof_1 == \\
	\t1	\{ 1 \mapsto (\{ 1\}, \propP \impliesPropVV \propQ, \ruleA), \\
	\t1	2 \mapsto (\{ 2 \}, \varProp(\propP), \ruleA), \\
	\t1	3 \mapsto (\{ 1, 2 \}, \varProp(\propQ), \ruleMPP(1,2)) \}
\end{zed}

\begin{example}
$Proof_1$ is an argument.

\begin{zed}
	Proof_1 \in Argument
\end{zed}

\end{example}

\subsection{$ArgumentLine$}

A argument is said to be {\it sound} or {\it valid} if each proof line in the argument
obeys the rules of derivation.
When assessing the validity of a proof line, we need to specify its line number within
the argument and examine its contents.
Let $ArgumentLine$ denote the set of all arguments and line numbers within it.

\begin{schema}{ArgumentLine}
	argument: Argument \\
	lineNumber: \nat_1
\where
	lineNumber \in \dom argument
\end{schema}

\begin{itemize}
\item The line number corresponds to a proof line within the argument.
\end{itemize}

\begin{example}
Line 3 in contained in proof $\mathbf{1}$.

\begin{zed}
	\LET argument == Proof_1; lineNumber == 3 @ \\
	\t1	ArgumentLine
\end{zed}

\end{example}

\subsection{$RuleOfAssumption$}

A proof line that uses the rule of assumption $\ruleA$ in an argument is sound if it depends only on itself.
Let $RuleOfAssumption$ denote the set of all argument lines that use the rule of assumption soundly.

\begin{schema}{RuleOfAssumption}
	ArgumentLine \\
	P: Prop
\where
	argument(lineNumber) = (\{ lineNumber \}, P,  \ruleA)
\end{schema}

\begin{itemize}
\item The line depends only on itself and the rule is the rule of assumptions $\ruleA$.
\end{itemize}

\subsection{$AssumptionArgumentLine$}

Let $AssumptionArgumentLine$ denote the set of all argument lines that are sound applications
of the rule of assumption.

\begin{zed}
	AssumptionArgumentLine \defs RuleOfAssumption \project ArgumentLine
\end{zed}

\begin{example}
Lines 1 and 2 of $Proof_1$ are sound applications of the rule of assumption.

\begin{zed}
	\LET	argument == Proof_1 @ \\
	\t1	\forall lineNumber : \{ 1, 2 \} @ \\
	\t2		AssumptionArgumentLine
\end{zed}

\end{example}

\subsection{$RuleOfMPP$}

A proof line that uses the rule $\ruleMPP(i,j)$ in an argument is sound if $i$ and $j$ are lines
that precede it, the proposition on line $i$ is an implication, the proposition on line $j$ is the antecedent
of the implication, the proposition of the proof line is the consequent of the implication,
and the proof line's assumptions is the union of the assumptions of lines $i$ and $j$.
Let $RuleOfMPP$ denote the set of all argument lines that use the rule of MPP soundly.

\begin{schema}{RuleOfMPP}
	ArgumentLine \\
	i, j: \nat_1 \\
	ProofLine_1 \\
	ProofLine_2 \\
	P, Q: Prop
\where
	i < lineNumber \land j < lineNumber
\also
	argument(i) = (assumptions_1, P \impliesProp Q, rule_1)
\also
	argument(j) = (assumptions_2, P, rule_2)
\also
	argument(lineNumber) = (assumptions_1 \cup assumptions_2, Q, \ruleMPP(i,j))
\end{schema}

\subsection{$MPPArgumentLine$}

Let $MPPArgumentLine$ denote the set of all argument lines that are sound applications
of the rule of MPP.

\begin{zed}
	MPPArgumentLine \defs RuleOfMPP \project ArgumentLine
\end{zed}

\begin{example}
Line 3 of $Proof_1$ is a sound application of the rule of MPP.

\begin{zed}
	\LET argument == Proof_1; \\
	\t1	lineNumber == 3 @ \\
	\t2		MPPArgumentLine
\end{zed}

\end{example}

\subsection{$SoundArgumentLine$}

An argument line is sound if it is a sound application of one of the rules of derivation.
Let $SoundArgumentLine$ denote the set of all sound argument lines.

\begin{zed}
	SoundArgumentLine \defs \\
	\t1	AssumptionArgumentLine \lor \\
	\t1	MPPArgumentLine
\end{zed}

\subsection{$Proof$}

A proof is an argument in which every line is sound.
Let $Proof$ denote the set of proofs.

\begin{zed}
	Proof == \{~ argument: Argument | \\
	\t1	\forall lineNumber: \dom argument @ \\
	\t2		SoundArgumentLine ~\}
\end{zed}

\begin{example}
$Proof_1$ is a proof.

\begin{zed}
	Proof_1 \in Proof
\end{zed}

\end{example}

\section{The Curry-Howard Correspondence}

The {\it Curry-Howard Correspondence} is an interpretation of propositions and proofs in terms of types.
To each proposition there correspondences a type.
The inhabitants of the type that corresponds to a proposition are proofs of that proposition.
The logical connectives that build up propositions correspond to type constructors.

To illustrate the correspondence, consider the implication logical connective $P \implies Q$.
It corresponds to the type constructor $P \fun Q$.
A proof $f$ of $P \implies Q$ corresponds to a function that maps any proof of $P$ to some proof of $Q$.

Of course, $P \fun Q$ is not a type in Z. 
It is a subset of the type $\power(P \cross Q)$.
Nevertheless, we'll press on and pretend that it is a Z type for now.

The rules of derivation correspond to the construction of a proof from assumptions.
For example the rule of MPP corresponds to function application.
Consider $Proof_1$.
It corresponds to the following.

\begin{schema}{CHProof1}[P,Q]
	f: P \fun Q \\
	x: P \\
	y: Q
\where
	y = f(x)
\end{schema}

\printbibliography

\end{document}